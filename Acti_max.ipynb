{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Acti_max.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8egtz9d97ZV",
        "outputId": "90e7b9c8-99c6-47b3-c9b2-0016455484f8"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.6/dist-packages (1.3.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKwMwI2wBEBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ee69a10b-7a98-43cd-bf94-ed6f994eb799"
      },
      "source": [
        "from __future__ import division\r\n",
        "from PIL import Image\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "from numpy.lib.stride_tricks import as_strided\r\n",
        "from heapq import nlargest\r\n",
        "\r\n",
        "import SPM_DVT as sd\r\n",
        "import itertools\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "import data_pull as dp\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "#import train_mnist as tm\r\n",
        "import output_mnist as om\r\n",
        "import load_mnist as lm\r\n",
        "import itertools\r\n",
        "import data_pull as dp\r\n",
        "import math\r\n",
        "import operator\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "from keras.datasets import mnist\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "\r\n",
        "import keras\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential,Model\r\n",
        "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.preprocessing.image import load_img\r\n",
        "from keras.preprocessing.image import img_to_array\r\n",
        "from keras.models import Model\r\n",
        "from matplotlib import pyplot\r\n",
        "from numpy import expand_dims\r\n",
        "from skimage import io\r\n",
        "\r\n",
        "def run_SPM(filter1, filter2, dir):\r\n",
        "    #directory\r\n",
        "    #array augmentation\r\n",
        "    array_1 = sd.resize_array(np.load(dir + filter1))\r\n",
        "    array_2 = sd.resize_array(np.load(dir + filter2))\r\n",
        "\r\n",
        "    #print(array_1.shape)\r\n",
        "\r\n",
        "    #degree\r\n",
        "    degree = sd.degree(array_1)\r\n",
        "    max_1 = np.amax(array_1)\r\n",
        "    max_2 = np.amax(array_2)\r\n",
        "    v1 = 512/max_1\r\n",
        "    v2 = 512/max_2\r\n",
        "    array_1 = v1 * array_1\r\n",
        "    array_2 = v2 * array_2\r\n",
        "\r\n",
        "    answers_dict, diff_dict = sd.pyr_all(array_1, array_2, degree)\r\n",
        "\r\n",
        "    final_dict = {}\r\n",
        "    for key in answers_dict:\r\n",
        "        if answers_dict[key] > [0]:\r\n",
        "            final_dict[key] = answers_dict[key]\r\n",
        "\r\n",
        "    #print(final_dict)\r\n",
        "    return dp.k_largest(40, final_dict,'y')\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c42186176179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mheapq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlargest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSPM_DVT\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SPM_DVT'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgS1h-DFAzGV"
      },
      "source": [
        "\r\n",
        "(X_train,Y_train),(X_test,Y_test)  = mnist.load_data()\r\n",
        "\r\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\r\n",
        "X_test  = X_test/255\r\n",
        "X_test  = X_test.astype('float')\r\n",
        "\r\n",
        "X_test = X_test[:1000,:,:,:]\r\n",
        "\r\n",
        "data_dir = 'data/'\r\n",
        "#classes = ['low/', 'high/','miss/','all/']\r\n",
        "classes = ['high/','miss/','all/']\r\n",
        "labels = [0,1,2,3,4,5,6,7,8,9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkLsiSczKC0w",
        "outputId": "c0371355-5b17-418e-c8b7-d19b1396f276"
      },
      "source": [
        "\r\n",
        "strategy = tf.distribute.MirroredStrategy()\r\n",
        "(X_train,Y_train),(X_test,Y_test)  = mnist.load_data()\r\n",
        "\r\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\r\n",
        "\r\n",
        "X_train = X_train/255\r\n",
        "X_train = X_train.astype('float')\r\n",
        "X_train[0].shape\r\n",
        "\r\n",
        "X_train = X_train[:4000,:,:,:]\r\n",
        "Y_train = Y_train[:4000]\r\n",
        "\r\n",
        "\"\"\"# **BUILD THE NETWORK**\"\"\"\r\n",
        "np.random.seed(0)\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(16,input_shape=(28,28,1),kernel_size=(3,3),activation='relu',padding='same'))\r\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))\r\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\r\n",
        "#model.add(MaxPooling2D())\r\n",
        "\r\n",
        "model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\r\n",
        "model.add(GlobalAveragePooling2D())\r\n",
        "model.add(Dense(10,activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\r\n",
        "\r\n",
        "model.fit(X_train,Y_train,batch_size=10,epochs=5,validation_split=0.1,shuffle=True)\r\n",
        "\r\n",
        "model_json = model.to_json()\r\n",
        "with open(\"model.json\", \"w\") as json_file:\r\n",
        "    json_file.write(model_json)\r\n",
        "\r\n",
        "model.save_weights('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 98,442\n",
            "Trainable params: 98,442\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "360/360 [==============================] - 2s 4ms/step - loss: 2.1627 - accuracy: 0.1840 - val_loss: 1.6719 - val_accuracy: 0.3250\n",
            "Epoch 2/5\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 1.3812 - accuracy: 0.5055 - val_loss: 1.2160 - val_accuracy: 0.5700\n",
            "Epoch 3/5\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.8354 - accuracy: 0.7316 - val_loss: 0.5997 - val_accuracy: 0.8125\n",
            "Epoch 4/5\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.8175 - val_loss: 0.3885 - val_accuracy: 0.8875\n",
            "Epoch 5/5\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.4812 - accuracy: 0.8499 - val_loss: 0.3914 - val_accuracy: 0.8775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bndNBFUS3d6K"
      },
      "source": [
        "##Spatial pyramid matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lb_t0xcKyd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660d4307-bad0-49ed-f1df-00c103317d8b"
      },
      "source": [
        "all_selected_labels = {}\r\n",
        "\r\n",
        "#user input into list - 4 5 -> ['4','5']\r\n",
        "user_input = [str(x) for x in input(\"Run pair-wise comparison on which label? (0-9)\").split()]\r\n",
        "\r\n",
        "#comparison run seperately for each input label\r\n",
        "for inputs in user_input:\r\n",
        "\r\n",
        "    dp.delete_folders(data_dir,classes)\r\n",
        "\r\n",
        "    #sampler - sample from each class\r\n",
        "    #other_arrays - low, high, missed\r\n",
        "    sampler, other_arrays = om.run()\r\n",
        "\r\n",
        "\r\n",
        "    sampled = []\r\n",
        "    for neurons in sampler[0]:\r\n",
        "        sampled.append(neurons[1])\r\n",
        "\r\n",
        "    lm.save_arrays(sampled, data_dir + sampler[1])\r\n",
        "    \r\n",
        "    for array in other_arrays:\r\n",
        "        dp.sort_arrays(array[0], inputs, data_dir + array[1])\r\n",
        "\r\n",
        "    all_freq = []\r\n",
        "    frequencies = {}\r\n",
        "\r\n",
        "    #Biulding Dictionaries of neuron frequency\r\n",
        "    for c in classes:\r\n",
        "        data_dir_list = os.listdir(data_dir + c)\r\n",
        "        all_neurons = []\r\n",
        "        for x,y in itertools.combinations(data_dir_list, 2):\r\n",
        "            new_neurons = run_SPM(x,y, data_dir+c)\r\n",
        "            all_neurons += new_neurons\r\n",
        "        print(c)\r\n",
        "        frequency = dp.CountFrequency(all_neurons)\r\n",
        "        all_freq.append(frequency)\r\n",
        "        #frequency2 = {k:v for k, v in frequency.items()}\r\n",
        "\r\n",
        "        # dictionary for the frequency of activations for most popular neurons\r\n",
        "        frequencies[c[:-1]] = frequency\r\n",
        "        #print(frequencies)\r\n",
        "    all_selected_labels[inputs] = frequencies\r\n",
        "\r\n",
        "print(all_selected_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run pair-wise comparison on which label? (0-9)2\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "high/\n",
            " 49 :  190\n",
            " 70 :  190\n",
            " 65 :  190\n",
            " 25 :  190\n",
            " 66 :  190\n",
            " 31 :  190\n",
            " 97 :  190\n",
            " 41 :  190\n",
            " 74 :  188\n",
            " 2 :  190\n",
            " 11 :  190\n",
            " 73 :  190\n",
            " 120 :  190\n",
            " 92 :  171\n",
            " 0 :  180\n",
            " 91 :  175\n",
            " 113 :  93\n",
            " 117 :  190\n",
            " 110 :  190\n",
            " 87 :  189\n",
            " 94 :  186\n",
            " 79 :  147\n",
            " 69 :  62\n",
            " 44 :  63\n",
            " 104 :  155\n",
            " 21 :  44\n",
            " 53 :  175\n",
            " 62 :  68\n",
            " 43 :  177\n",
            " 102 :  143\n",
            " 14 :  154\n",
            " 59 :  137\n",
            " 125 :  183\n",
            " 19 :  171\n",
            " 85 :  17\n",
            " 108 :  108\n",
            " 103 :  11\n",
            " 22 :  32\n",
            " 28 :  90\n",
            " 60 :  155\n",
            " 30 :  81\n",
            " 57 :  27\n",
            " 5 :  39\n",
            " 58 :  46\n",
            " 100 :  50\n",
            " 52 :  64\n",
            " 80 :  49\n",
            " 83 :  15\n",
            " 33 :  28\n",
            " 50 :  73\n",
            " 82 :  118\n",
            " 67 :  106\n",
            " 101 :  73\n",
            " 37 :  138\n",
            " 18 :  115\n",
            " 64 :  77\n",
            " 55 :  95\n",
            " 71 :  41\n",
            " 126 :  50\n",
            " 13 :  73\n",
            " 63 :  11\n",
            " 56 :  45\n",
            " 38 :  2\n",
            " 12 :  97\n",
            " 114 :  85\n",
            " 45 :  4\n",
            " 9 :  20\n",
            " 107 :  17\n",
            " 96 :  7\n",
            " 48 :  1\n",
            " 61 :  1\n",
            " 88 :  10\n",
            " 3 :  6\n",
            " 27 :  2\n",
            "miss/\n",
            " 70 :  190\n",
            " 49 :  190\n",
            " 65 :  190\n",
            " 25 :  190\n",
            " 66 :  190\n",
            " 97 :  190\n",
            " 41 :  190\n",
            " 31 :  190\n",
            " 91 :  186\n",
            " 19 :  170\n",
            " 117 :  142\n",
            " 125 :  189\n",
            " 2 :  190\n",
            " 73 :  187\n",
            " 87 :  131\n",
            " 0 :  180\n",
            " 43 :  189\n",
            " 92 :  172\n",
            " 11 :  189\n",
            " 74 :  164\n",
            " 28 :  129\n",
            " 120 :  190\n",
            " 55 :  126\n",
            " 59 :  142\n",
            " 37 :  135\n",
            " 60 :  160\n",
            " 104 :  162\n",
            " 18 :  106\n",
            " 113 :  103\n",
            " 62 :  88\n",
            " 110 :  185\n",
            " 94 :  151\n",
            " 14 :  104\n",
            " 71 :  134\n",
            " 12 :  137\n",
            " 21 :  59\n",
            " 69 :  87\n",
            " 126 :  90\n",
            " 64 :  121\n",
            " 101 :  159\n",
            " 22 :  99\n",
            " 103 :  77\n",
            " 102 :  131\n",
            " 108 :  55\n",
            " 53 :  139\n",
            " 45 :  30\n",
            " 48 :  15\n",
            " 44 :  63\n",
            " 79 :  112\n",
            " 13 :  28\n",
            " 9 :  62\n",
            " 85 :  27\n",
            " 114 :  64\n",
            " 107 :  84\n",
            " 5 :  60\n",
            " 67 :  84\n",
            " 52 :  27\n",
            " 30 :  26\n",
            " 82 :  45\n",
            " 61 :  4\n",
            " 50 :  56\n",
            " 56 :  15\n",
            " 3 :  18\n",
            " 63 :  5\n",
            " 88 :  4\n",
            " 100 :  18\n",
            " 124 :  2\n",
            " 58 :  16\n",
            " 80 :  5\n",
            " 33 :  14\n",
            " 83 :  9\n",
            " 57 :  21\n",
            " 27 :  2\n",
            " 112 :  1\n",
            " 93 :  1\n",
            " 77 :  1\n",
            " 6 :  1\n",
            "all/\n",
            " 70 :  190\n",
            " 49 :  190\n",
            " 65 :  190\n",
            " 97 :  190\n",
            " 91 :  190\n",
            " 125 :  186\n",
            " 60 :  186\n",
            " 59 :  143\n",
            " 25 :  190\n",
            " 19 :  175\n",
            " 87 :  148\n",
            " 50 :  109\n",
            " 28 :  159\n",
            " 67 :  149\n",
            " 66 :  170\n",
            " 14 :  147\n",
            " 56 :  89\n",
            " 101 :  184\n",
            " 107 :  138\n",
            " 64 :  167\n",
            " 31 :  153\n",
            " 126 :  129\n",
            " 82 :  129\n",
            " 3 :  95\n",
            " 37 :  179\n",
            " 112 :  61\n",
            " 124 :  42\n",
            " 117 :  149\n",
            " 71 :  184\n",
            " 18 :  159\n",
            " 41 :  152\n",
            " 2 :  153\n",
            " 93 :  34\n",
            " 114 :  125\n",
            " 55 :  182\n",
            " 13 :  94\n",
            " 77 :  19\n",
            " 121 :  12\n",
            " 63 :  79\n",
            " 43 :  190\n",
            " 11 :  144\n",
            " 74 :  108\n",
            " 120 :  154\n",
            " 73 :  122\n",
            " 94 :  51\n",
            " 0 :  119\n",
            " 92 :  88\n",
            " 102 :  57\n",
            " 53 :  78\n",
            " 110 :  115\n",
            " 79 :  28\n",
            " 85 :  75\n",
            " 104 :  97\n",
            " 88 :  41\n",
            " 38 :  31\n",
            " 103 :  33\n",
            " 9 :  53\n",
            " 100 :  20\n",
            " 62 :  35\n",
            " 5 :  37\n",
            " 21 :  32\n",
            " 22 :  41\n",
            " 113 :  43\n",
            " 44 :  30\n",
            " 69 :  40\n",
            " 45 :  18\n",
            " 108 :  7\n",
            " 33 :  3\n",
            " 57 :  4\n",
            " 58 :  2\n",
            " 80 :  1\n",
            " 30 :  1\n",
            " 48 :  6\n",
            " 52 :  1\n",
            " 6 :  1\n",
            " 12 :  124\n",
            " 1 :  2\n",
            "{'2': {'high': {49: 190, 70: 190, 65: 190, 25: 190, 66: 190, 31: 190, 97: 190, 41: 190, 74: 188, 2: 190, 11: 190, 73: 190, 120: 190, 92: 171, 0: 180, 91: 175, 113: 93, 117: 190, 110: 190, 87: 189, 94: 186, 79: 147, 69: 62, 44: 63, 104: 155, 21: 44, 53: 175, 62: 68, 43: 177, 102: 143, 14: 154, 59: 137, 125: 183, 19: 171, 85: 17, 108: 108, 103: 11, 22: 32, 28: 90, 60: 155, 30: 81, 57: 27, 5: 39, 58: 46, 100: 50, 52: 64, 80: 49, 83: 15, 33: 28, 50: 73, 82: 118, 67: 106, 101: 73, 37: 138, 18: 115, 64: 77, 55: 95, 71: 41, 126: 50, 13: 73, 63: 11, 56: 45, 38: 2, 12: 97, 114: 85, 45: 4, 9: 20, 107: 17, 96: 7, 48: 1, 61: 1, 88: 10, 3: 6, 27: 2}, 'miss': {70: 190, 49: 190, 65: 190, 25: 190, 66: 190, 97: 190, 41: 190, 31: 190, 91: 186, 19: 170, 117: 142, 125: 189, 2: 190, 73: 187, 87: 131, 0: 180, 43: 189, 92: 172, 11: 189, 74: 164, 28: 129, 120: 190, 55: 126, 59: 142, 37: 135, 60: 160, 104: 162, 18: 106, 113: 103, 62: 88, 110: 185, 94: 151, 14: 104, 71: 134, 12: 137, 21: 59, 69: 87, 126: 90, 64: 121, 101: 159, 22: 99, 103: 77, 102: 131, 108: 55, 53: 139, 45: 30, 48: 15, 44: 63, 79: 112, 13: 28, 9: 62, 85: 27, 114: 64, 107: 84, 5: 60, 67: 84, 52: 27, 30: 26, 82: 45, 61: 4, 50: 56, 56: 15, 3: 18, 63: 5, 88: 4, 100: 18, 124: 2, 58: 16, 80: 5, 33: 14, 83: 9, 57: 21, 27: 2, 112: 1, 93: 1, 77: 1, 6: 1}, 'all': {70: 190, 49: 190, 65: 190, 97: 190, 91: 190, 125: 186, 60: 186, 59: 143, 25: 190, 19: 175, 87: 148, 50: 109, 28: 159, 67: 149, 66: 170, 14: 147, 56: 89, 101: 184, 107: 138, 64: 167, 31: 153, 126: 129, 82: 129, 3: 95, 37: 179, 112: 61, 124: 42, 117: 149, 71: 184, 18: 159, 41: 152, 2: 153, 93: 34, 114: 125, 55: 182, 13: 94, 77: 19, 121: 12, 63: 79, 43: 190, 11: 144, 74: 108, 120: 154, 73: 122, 94: 51, 0: 119, 92: 88, 102: 57, 53: 78, 110: 115, 79: 28, 85: 75, 104: 97, 88: 41, 38: 31, 103: 33, 9: 53, 100: 20, 62: 35, 5: 37, 21: 32, 22: 41, 113: 43, 44: 30, 69: 40, 45: 18, 108: 7, 33: 3, 57: 4, 58: 2, 80: 1, 30: 1, 48: 6, 52: 1, 6: 1, 12: 124, 1: 2}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVlP7N6cgIAt"
      },
      "source": [
        "##TF-IDF\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70tP7OVGOEKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d093354-b54e-4adb-d45d-ca5a280fba36"
      },
      "source": [
        "\r\n",
        "max_high = {}\r\n",
        "max_miss = {}\r\n",
        "\r\n",
        "if len(all_selected_labels) > 0:\r\n",
        "    tf_idf_value = input(\"Run tf-idf? (Y/N)\")\r\n",
        "    if tf_idf_value == 'Y' or tf_idf_value == 'y':\r\n",
        "        for inputs in all_selected_labels:\r\n",
        "            for key in inputs:\r\n",
        "                frequencies = all_selected_labels[key]\r\n",
        "                #print(key)\r\n",
        "                high = frequencies['high']\r\n",
        "                #low = frequencies['low']\r\n",
        "                alll = frequencies['all'] \r\n",
        "                miss = frequencies['miss']\r\n",
        "\r\n",
        "                group = [high, miss, alll]\r\n",
        "\r\n",
        "\r\n",
        "                #### Build of all similarities found during pair-wise compairison for use during tf-idf calculations ###\r\n",
        "                main = {}\r\n",
        "                for dictionary in all_freq:\r\n",
        "                    main = dp.mergeDict(main, dictionary)\r\n",
        "\r\n",
        "                act_frequency = sum(all_freq[0].values()) + sum(all_freq[1].values()) + sum(all_freq[2].values())\r\n",
        "\r\n",
        "                tf_high = {}\r\n",
        "                tf_miss = {}\r\n",
        "                tf_all ={}\r\n",
        "                for key in main:\r\n",
        "                    count = 0\r\n",
        "                    for g in group:\r\n",
        "                        #if key in g and g[key] >= 145:\r\n",
        "                        if key in g:\r\n",
        "                            count +=1\r\n",
        "                    #print(count)\r\n",
        "                    #print('key: ', key)\r\n",
        "                    for f in frequencies:\r\n",
        "\r\n",
        "                        #if key in frequencies[f] and frequencies[f][key] >= 145:\r\n",
        "                        if key in frequencies[f]:\r\n",
        "                            #print(f)\r\n",
        "                            #print(f, ' value: ' , (frequencies[f][key] / sum(frequencies[f].values()))  * np.log((3 / count) + 1))\r\n",
        "                            if f == 'high':\r\n",
        "                                tf_high[key] = (frequencies[f][key] / sum(frequencies[f].values()))  * np.log((3 / count) + 1)\r\n",
        "                            if f == 'miss':\r\n",
        "                                tf_miss[key] = (frequencies[f][key] / sum(frequencies[f].values()))  * np.log((3 / count) + 1)\r\n",
        "                            if f == 'all':\r\n",
        "                                tf_all[key] = (frequencies[f][key] / sum(frequencies[f].values()))  * np.log((3 / count) + 1)    \r\n",
        "                \r\n",
        "                #print(tf_high)\r\n",
        "                #print(tf_miss)\r\n",
        "                #print(tf_all)\r\n",
        "\r\n",
        "                for key in tf_all:\r\n",
        "                    if key in tf_miss:\r\n",
        "                        if tf_all[key] * 1.2 > tf_miss[key]:\r\n",
        "                            del tf_miss[key]\r\n",
        "                    if key in tf_high:\r\n",
        "                        if tf_all[key] * 1.2 > tf_high[key]:\r\n",
        "                            del tf_high[key]\r\n",
        "                \r\n",
        "                act_frequency = sum(alll.values())\r\n",
        "\r\n",
        "                Missing_Activations = []\r\n",
        "                #print('inputs: ', inputs)\r\n",
        "                max_list_high = []\r\n",
        "                max_list_miss = []\r\n",
        "                m_high = 0\r\n",
        "                m_miss = 0\r\n",
        "\r\n",
        "                #print('high')\r\n",
        "                for key in tf_high:\r\n",
        "                    #if key not in low.keys() and key not in miss.keys():\r\n",
        "                    #print('high')\r\n",
        "                    if key not in tf_miss:\r\n",
        "                        #print(key, tf_high[key])\r\n",
        "                        if tf_high[key] > m_high:\r\n",
        "                            max_high[inputs] = [inputs, key, tf_high[key]]\r\n",
        "                        max_list_high.append([inputs, key, tf_high[key]])\r\n",
        "\r\n",
        "                    elif tf_high[key] > tf_miss[key]:\r\n",
        "                        if tf_high[key] > m_high:\r\n",
        "                            max_high[inputs] = [inputs, key, tf_high[key]]\r\n",
        "                        #print(key, tf_high[key])\r\n",
        "                        max_list_high.append([inputs, key, tf_high[key]])\r\n",
        "\r\n",
        "                #print('miss')\r\n",
        "                for key in tf_miss:\r\n",
        "                    #print('miss')\r\n",
        "                    if key not in tf_high:\r\n",
        "                        #print(key, tf_miss[key])\r\n",
        "                        if tf_miss[key] > m_miss:\r\n",
        "                            max_miss[inputs] = [inputs, key, tf_miss[key]]\r\n",
        "                        max_list_miss.append([inputs, key, tf_miss[key]])\r\n",
        "                    elif tf_miss[key] > tf_miss[key]:\r\n",
        "                        #print(key, tf_miss[key])\r\n",
        "                        if tf_miss[key] > m_miss:\r\n",
        "                            max_miss[inputs] = [inputs, key, tf_miss[key]]\r\n",
        "                        max_list_miss.append([inputs, key, tf_miss[key]])\r\n",
        "\r\n",
        "\r\n",
        "print('only prints max value')\r\n",
        "print('{input class, [input class, tf_idf value]}')\r\n",
        "print('max_high: ', max_high)\r\n",
        "print('max_miss: ', max_miss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run tf-idf? (Y/N)y\n",
            "only prints max value\n",
            "{input class, [input class, tf_idf value]}\n",
            "max_high:  {'2': ['2', 96, 0.0012768500694525307]}\n",
            "max_miss:  {'2': ['2', 48, 0.0013684137547248195]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY4ua_cg2fjf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}